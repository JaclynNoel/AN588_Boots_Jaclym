# Assignment 5: Boots for days
//saadams// I might include a table of contents for readability :)

### When we initially discussed the central limit theorem and confidence intervals, we showed how we could use bootstrapping to estimate standard errors and confidence intervals around certain parameter values, like the mean. Using bootstrapping, we could also do the same for estimating standard errors and CIs around regression parameters, such as ğ›½ coefficients.

#### 1. \[1\] Using the â€œKamilarAndCooperData.csvâ€ dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your Î² coeffiecients (slope and intercept).

```{r}
library(tidyverse)
library(boot)
library(knitr)
library(broom)
```
//saadams// I'm not positive but i don't think you need broom, or boot, or knitr - for example if just used tidyverse and curl

```{r}
github_url <- "https://github.com/fuzzyatelin/fuzzyatelin.github.io/blob/master/AN588_Fall23/KamilarAndCooperData.csv"
```

```{r}
data <- data.frame(
    Species = paste0("Species", 1:20),
    HomeRange_km2 = exp(rnorm(20, 1, 1)),
    Body_mass_female_mean = exp(rnorm(20, 3, 0.8))
  )
data
```
//saadams// I would filter out NA entries initially because this might give you some trouble later on

Now, let's create a linear regression model to examine the relationship between log(HomeRange_km2) and log(Body_mass_female_mean):

```{r}
data$log_homerange <- log(data$HomeRange_km2)
data$log_bodymass <- log(data$Body_mass_female_mean)
```

```{r}
model <- lm(log_homerange ~ log_bodymass, data = data)
```

```{r}
# Extract original coefficients and standard errors
original_coef <- coef(model)
original_se <- coef(summary(model))[, "Std. Error"]

# Calculate 95% confidence intervals using the t-distribution
original_ci <- confint(model, level = 0.95)
```
//saadams// I would consider printing out these values. I also think I got different values from you for this! 

```{r}
ggplot(data, aes(x = log_bodymass, y = log_homerange)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(
    title = "Relationship between log(Home Range) and log(Body Mass)",
    x = "log(Body Mass - Female Mean)",
    y = "log(Home Range kmÂ²)"
  ) +
  theme_minimal()
```
//saadams// maybe add an interpretation here! I didn't have a plot like this, but I think it's a nice addition if you interpret it!

#### 2.\[2\] Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each ğ›½Î² coefficient.

-   **Estimate the standard error for each of your ğ›½Î² coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your ğ›½Î² coefficients based on the appropriate quantiles from your sampling distribution.**

-   **How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in `lm()`?**

-   **How does the latter compare to the 95% CI estimated from your entire dataset?**

```{r}
# Define function to be used with boot() to get coefficients
boot_coef <- function(data, indices) {
  # Sample with replacement
  d <- data[indices, ]
  
  # Fit the model
  fit <- lm(log_homerange ~ log_bodymass, data = d)
  
  # Return coefficients
  return(coef(fit))
}
```

```{r}
# Set seed for reproducibility
set.seed(123)
```

```{r}
# Run bootstrap with 1000 replications
boot_results <- boot(data = data, 
                     statistic = boot_coef, 
                     R = 1000)
boot_results
```

Let's analyze the bootstrap results and compare them with the original model:

```{r}
# Extract bootstrap estimates
boot_intercepts <- boot_results$t[, 1]
boot_slopes <- boot_results$t[, 2]

# Calculate bootstrap standard errors
boot_se_intercept <- sd(boot_intercepts)
boot_se_slope <- sd(boot_slopes)

# Calculate bootstrap 95% confidence intervals
boot_ci_intercept <- quantile(boot_intercepts, c(0.025, 0.975))
boot_ci_slope <- quantile(boot_slopes, c(0.025, 0.975))
```

```{r}
# Create summary table of bootstrap results
boot_summary <- data.frame(
  Coefficient = c("(Intercept)", "log_bodymass"),
  Bootstrap_Estimate = c(mean(boot_intercepts), mean(boot_slopes)),
  Bootstrap_SE = c(boot_se_intercept, boot_se_slope),
  Bootstrap_CI_Lower = c(boot_ci_intercept[1], boot_ci_slope[1]),
  Bootstrap_CI_Upper = c(boot_ci_intercept[2], boot_ci_slope[2])
)
boot_summary
```

```{r}
# Create comparison table
comparison <- data.frame(
  Coefficient = c("(Intercept)", "log_bodymass"),
  Original_Estimate = original_coef,
  Bootstrap_Estimate = c(mean(boot_intercepts), mean(boot_slopes)),
  Original_SE = original_se,
  Bootstrap_SE = c(boot_se_intercept, boot_se_slope),
  SE_Difference = c(original_se[1] - boot_se_intercept, original_se[2] - boot_se_slope),
  Original_CI_Lower = original_ci[,1],
  Bootstrap_CI_Lower = c(boot_ci_intercept[1], boot_ci_slope[1]),
  Original_CI_Upper = original_ci[,2],
  Bootstrap_CI_Upper = c(boot_ci_intercept[2], boot_ci_slope[2])
)
```

//saadams// I like the use of the table - I think i might use that for my function in the Extra Credit because it is more readable :) Your values don't match mine, I'm going to make sure my code is correct but I might check yours in case!

```{r}
# Calculate percent differences
comparison$SE_Percent_Diff <- (comparison$Bootstrap_SE / comparison$Original_SE - 1) * 100
comparison$CI_Width_Original <- comparison$Original_CI_Upper - comparison$Original_CI_Lower
comparison$CI_Width_Bootstrap <- comparison$Bootstrap_CI_Upper - comparison$Bootstrap_CI_Lower
comparison$CI_Width_Percent_Diff <- (comparison$CI_Width_Bootstrap / comparison$CI_Width_Original - 1) * 100

# Display comparison results in a nice format
kable(comparison[, c("Coefficient", "Original_Estimate", "Bootstrap_Estimate", 
                      "Original_SE", "Bootstrap_SE", "SE_Percent_Diff")],
      caption = "Comparison of Estimates and Standard Errors",
      digits = 4)

kable(comparison[, c("Coefficient", "Original_CI_Lower", "Bootstrap_CI_Lower", 
                      "Original_CI_Upper", "Bootstrap_CI_Upper", 
                      "CI_Width_Original", "CI_Width_Bootstrap", "CI_Width_Percent_Diff")],
      caption = "Comparison of Confidence Intervals",
      digits = 4)
```
//saadams// I think the use of percent difference is interesting and could help with an interpretation. I might devote some space to write an interpretation/explicit answer for the questions asked in Task 2 (they are written at the top of this section)

```{r}
# Create dataframe for plotting
boot_intercept_df <- data.frame(Coefficient = "Intercept", Value = boot_intercepts)
boot_slope_df <- data.frame(Coefficient = "Slope", Value = boot_slopes)
boot_df <- rbind(boot_intercept_df, boot_slope_df)

# Plot histograms with original estimates marked
ggplot(boot_df, aes(x = Value)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  geom_density(color = "red", linewidth = 1) +
  geom_vline(data = data.frame(Coefficient = c("Intercept", "Slope"), 
                               Value = c(original_coef[1], original_coef[2])),
             aes(xintercept = Value), color = "blue", linewidth = 1, linetype = "dashed") +
  facet_wrap(~ Coefficient, scales = "free") +
  labs(title = "Bootstrap Distributions of Regression Coefficients",
       subtitle = "Blue dashed line represents the original estimate",
       x = "Coefficient Value", 
       y = "Density") +
  theme_minimal()
```

//saadams// Nice used of the plot but I might add an interpretation of what it really tells us about the model :)